% !TEX root = ../ts_pset_main.tex

\Opensolutionfile{solution_file}[solutions/sols_005]
% в квадратных скобках фактическое имя файла

\chapter{Автокорреляция ошибок в линейной модели}

\begin{problem}
Билл Гейтс оценил модель $y_t=\beta_1 + \beta_2 t + \beta_3 y_{t-1} + \e_t$ с помощью МНК. Значение статистики Дарбина-Уотсона оказалось равно $DW=0.55$. Какой из этого следует вывод об автокорреляции ошибок первого порядка?


\begin{sol}
В данном случае статистика $DW$ не применима, так как есть лаг $y_{t-1}$ среди регрессоров.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим модель $y_t=\beta x_t +\e_t$, где $\e_1=u_1$ и $\e_t=u_t+u_{t-1}$ при $t\geq 2$. Случайные величины $u_i$ независимы с $\E(u_i)=0$ и $\Var(u_i)=\sigma^2$.
\begin{enumerate}
\item Найдите $\Var(\e_t)$
\item Являются ли ошибки $\e_t$ гетероскедастичными?
\item Найдите $\Cov(\e_i,\e_j)$
\item Являются ли ошибки $\e_t$ автокоррелированными?
\item Как выглядит матрица $\Var(\e)$?
\item Рассмотрим оценку
\[
\hb=\frac{\sum x_i y_i}{\sum x_i^2}
\]
Является ли она несмещенной для $\beta$? Является ли она эффективной в классе линейных по $y$ несмещенных оценок?
\item Если приведенная $\hb$ не является эффективной, то приведите формулу для эффективной оценки.
\end{enumerate}



\begin{sol}
\begin{enumerate}
\item $\E(\e_t)=0$, $\Var(\e_1)=\sigma^2$, $\Var(\e_t)=2\sigma^2$ при $t\geq 2$.  Гетероскедастичная.
\item $\Cov(e_t,e_{t+1})=\sigma^2$. Автокоррелированная.
\item $\hb$ --- несмещенная, неэффективная
\item Более эффективной будет $\hb_{gls}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где
\[
X=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
\]

Матрица $V$ известна с точностью до константы $\sigma^2$, но в формуле для $\hb_{gls}$ неизвестная $\sigma^2$ сократится.

Другой способ построить эффективную оценку --- применить МНК к преобразованным наблюдениям, т.е. $\hb_{gls}=\frac{\sum x'_i y'_i}{\sum x_i^{\prime 2}}$, где $y'_1=y_1$, $x'_1=x_1$, $y'_t=y_t-y_{t-1}$, $x'_t=x_t-x_{t-1}$ при $t\geq 2$.
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
Имеются данные $y=(1,\, 2,\, 0,\,  0,\, 2,\, 1)$. Предполагая модель с автокоррелированной ошибкой, $y_t=\mu+\e_t$, где $\e_t=\rho \e_{t-1}+u_t$ с помощью трёх тестов проверьте гипотезы
$H_0$: $\rho=0$,
$H_0$: $\mu=0$,
$H_0$: $\begin{cases}
\rho=0 \\
\mu = 0 \\
\sigma^2=1
\end{cases}$



\begin{sol}

Для простоты закроем глаза на малое количество наблюдений и как индейцы пираха будем считать, что пять --- это много.

\end{sol}
\end{problem}


\begin{problem}
Рассматривается модель $y_t = \mu + \varepsilon_t$, $t = 1,\ldots,T$, где $\varepsilon_t = \rho \varepsilon_{t-1} + u_t$, случайные величины $\varepsilon_0, u_1,\dots,u_T$ независимы, причем $\varepsilon_0 \sim N(0,\sigma^2/(1 - \rho^2))$, $u_t \sim N(0,\sigma^2)$. Имеются наблюдения $y' = (1, 2, 0, 0, 1)$.
\begin{enumerate}
  \item Выпишите функцию правдоподобия
  \[
  \mathrm{L}(\mu, \rho, \sigma^2) = f_{Y_1}(y_1)\prod_{t=2}^{T}f_{Y_t|Y_{t-1}}(y_t|y_{t-1}).
  \]
  \item Найдите оценки неизвестных параметров модели максимизируя условную функцию правдоподобия
  \[
  \mathrm{L}(\mu, \rho, \sigma^2|Y_1 = y_1) = \prod_{t=2}^{T}f_{Y_t|Y_{t-1}}(y_t|y_{t-1})
  \]
\end{enumerate}


\begin{sol}
1. Поскольку имеют место соотношения $\varepsilon_1 = \rho \varepsilon_0 + u_1$ и $Y_1 =\mu + \varepsilon_1$, то из условия задачи получаем, что $\varepsilon_1 \sim N(0,\sigma^2 / (1 - \rho^2))$
и $Y_1 \sim N(\mu,\sigma^2 / (1 - \rho^2))$. Поэтому
\[
f_{Y_1}(y_1) = \frac{1}{\sqrt{2\pi\sigma^2/(1-\rho^2)}}\exp{\left(-\frac{(y_1 - \mu)^2}{2\sigma^2/(1 - \rho^2)}\right)}.
\]

Далее, найдем $f_{Y_2|Y_1}(y_2|y_1)$. Учитывая, что $Y_2 = \rho Y_1 + (1- \rho) \mu + u_2$, получаем $Y_2|\{Y_1 = y_1\} \sim N(\rho y_1 + (1- \rho) \mu, \sigma^2)$. Значит,
\[
f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_2 - \rho y_1 - (1- \rho) \mu)^2}{2\sigma^2}\right)}.
\]

Действуя аналогично, получаем, что для всех $t \geq 2$ справедлива формула
\[
f_{Y_{t}|Y_{t-1}}(y_{t}|y_{t-1}) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_{t} - \rho y_{t-1} - (1- \rho) \mu)^2}{2\sigma^2}\right)}.
\]

Таким образом, находим функцию правдоподобия
\[
\mathrm{L}(\mu, \rho, \sigma^2) = f_{Y_T,\ldots,Y_1}(y_T,\dots,y_1) = f_{Y_1}(y_1)\prod_{t=2}^{T}f_{Y_t|Y_{t-1}}(y_t|y_{t-1}) \text{,}
\]
где $f_{Y_1}(y_1)$ и $f_{Y_t|Y_{t-1}}(y_t|y_{t-1})$ получены выше.

2. Для нахождения неизвестных параметров модели запишем логарифмическую условную функцию правдоподобия:
\[
l(\mu, \rho, \sigma^2|Y_1 = y_1) = \sum_{t=2}^{T}\log{f_{Y_t|Y_{t-1}}(y_t|y_{t-1})} =
\]
\[
=-\frac{T-1}{2} \log(2 \pi) - \frac{T-1}{2} \log{\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^{T}(y_t - \rho y_{t-1} - (1 - \rho) \mu)^2 \text{.}
\]

Найдем производные функции $l(\mu, \rho, \sigma^2|Y_1 = y_1)$ по неизвестным параметрам:
\[
\frac{\partial l}{\partial \mu} = -\frac{1}{2\sigma^2} \sum_{t=2}^{T} 2(y_t - \rho y_{t-1} - (1 - \rho) \mu) \cdot (\rho - 1) \text{,}
\]
\[
\frac{\partial l}{\partial \rho} = -\frac{1}{2\sigma^2} \sum_{t=2}^{T} 2(y_t - \rho y_{t-1} - (1 - \rho) \mu) \cdot (\mu - y_{t-1}) \text{,}
\]
\[
\frac{\partial l}{\partial {\sigma^2}} =  - \frac{T-1}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{t=2}^{T}(y_t - \rho y_{t-1} - (1 - \rho) \mu)^2 \text{.}
\]

Оценки неизвестных параметров модели могут быть получены как решение следующей системы уравнений:
\[
\left\{
  \begin{aligned}
    \frac{\partial l}{\partial \mu} = 0 \text{,} \\
    \frac{\partial l}{\partial \rho} = 0 \text{,} \\
    \frac{\partial l}{\partial {\sigma^2}} = 0 \text{.}
  \end{aligned}
\right.
\]

Из первого уравнения системы получаем, что
\[
\sum_{t=2}^{T}y_{t} - \hat{\rho} \sum_{t=2}^{T}y_{t-1} = (T - 1) (1- \hat{\rho}) \hat{\mu} \text{,}
\]
откуда
\[
\hat{\mu} = \frac{\sum_{t=2}^{T}y_{t} - \hat{\rho} \sum_{t=2}^{T}y_{t-1}}{(T - 1) (1- \hat{\rho})} = \frac{3 - \hat{\rho} \cdot 3}{4\cdot(1-\hat{\rho})} = \frac{3}{4} \text{.}
\]

Далее, если второе уравнение системы переписать в виде
\[
\sum_{t=2}^{T}(y_t - \hat{\mu} - \hat{\rho} (y_{t-1} - \hat{\mu}))(y_{t-1} - \hat{\mu}) = 0 \text{,}
\]
то легко видеть, что
\[
\hat{\rho} = \frac{\sum_{t=2}^{T}(y_t - \hat{\mu})(y_{t-1} - \hat{\mu})}{\sum_{t=2}^{T}(y_{t-1} - \hat{\mu})^2} \text{.}
\]
Следовательно, $\hat{\rho} =-1/11= -0.0909$.

Наконец, из третьего уравнения системы
\[
\hs^2 =\frac{1}{T-1} \sum_{t=2}^{T}(y_t - \hat{\rho} y_{t-1} - (1 - \hat{\rho}) \hat{\mu})^2 \text{.}
\]
Значит, $\hs^2 = 165/242= 0.6818$. Ответы: $\hat{\mu} = 3/4= 0.75$, $\hat{\rho} = -1/11=-0.0909$, $\hs^2 =165/242=0.6818$.
\end{sol}
\end{problem}




\begin{problem}
Остаются ли в условиях автокорреляции МНК-
оценки в линейной модели несмещёнными? Состоятельными?
\begin{sol}
Несмещёнными остаются. Состоятельными не всегда остаются, например, состоятельность исчезает, если все случайные ошибки тождественно равны между собой.

\end{sol}
\end{problem}



\begin{problem}
Продавец мороженного оценил динамическую модель объёмов продаж:
\[
\ln \hat{Q}_t=26.7 + 0.2\ln \hat{Q}_{t-1}-0.6\ln P_t
\]
Здесь $Q_t$ --- число проданных в день $t$ вафельных стаканчиков, а $P_t$ --- цена одного стаканчика в рублях. Продавец также рассчитал остатки $\hat{e}_t$.
\begin{enumerate}
\item Чему, согласно полученным оценкам, равна долгосрочная эластичность объёма продаж по цене?
\item Предположим, что продавец решил проверить наличие автокорреляции первого порядка с помощью теста Бройша-Годфри. Выпишите уравнение регрессии, которое он должен оценить.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Пусть $u_t$ --- независимые нормальные случайные величины с
математическим ожиданием $0$ и дисперсией $\sigma^2$. Известно, что $\e_1=u_1$, $\e_t=u_1+u_2+\ldots+u_t$. Рассмотрим модель $y_t=\beta_1+\beta_2 x_t + \e_t$.

\begin{enumerate}
\item Найдите $\Var(\e_t)$, $\Cov(\e_t,\e_s)$, $\Var(\e)$
\item Являются ли ошибки $\e_t$ гетероскедастичными?
\item Являются ли ошибки $\e_t$ автокоррелированными?
\item Предложите более эффективную оценку вектора коэффициентов регрессии по сравнению МНК-оценкой.
\item Результаты предыдущего пункта подтвердите симуляциями Монте-Карло на компьютере.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Ошибки в модели $y_t=\beta_1+\beta_2 x_{t}+\e_t$ являются автокоррелированными первого порядка, $\e_t=\rho \e_{t-1}+u_t$. Шаман-эконометрист Ойуун выполняет два камлания-преобразования. Поясните смысл камланий:
\begin{enumerate}
\item Камлание А, при $t\geq 2$, Ойуун преобразует уравнение к виду $y_t-\rho y_{t-1}=\beta_1(1-\rho)+ \beta_2(x_t-\rho x_{t-1})+\e_t-\rho \e_{t-1}$
\item Камлание Б, при $t=1$, Ойуун преобразует уравнение к виду $\sqrt{1-\rho^2}y_1=\sqrt{1-\rho^2}\beta_1+\sqrt{1-\rho^2}\beta_2 x_1+\sqrt{1-\rho^2}\e_1$.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим модель $y_t=\beta_1+\beta_2 x_{t1}+\ldots+\beta_k x_{tk}+\e_t$, где $\e_t$ подчиняются автокорреляционной схеме первого порядка, т.е.
\begin{enumerate}
\item $\e_t=\rho \e_{t-1}+u_t$, $-1<\rho<1$
\item $\Var(\e_t)=const$, $\E(\e_t)=const$
\item $\Var(u_t)=\sigma^2$, $\E(u_t)=0$
\item Величины $u_t$ независимы между собой
\item Величины $u_t$ и $\e_s$ независимы, если $t\geq s$
\end{enumerate}
Найдите:
\begin{enumerate}
\item $\E(\e_t)$, $\Var(\e_t)$
\item $\Cov(\e_t,\e_{t+h})$
\item $\Corr(\e_t,\e_{t+h})$
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $\E(\e_t)=0$, $\Var(\e_t)=\sigma^2/(1-\rho^2)$
\item $\Cov(\e_t,\e_{t+h})=\rho^h\cdot \sigma^2/(1-\rho^2)$
\item $\Corr(\e_t,\e_{t+h})=\rho^h$
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Рассматривается модель $y_t=\beta_1+\beta_2 x_{t1}+\ldots+\beta_k x_{tk}+\e_t$. Ошибки $\e_t$ гомоскедастичны, но в них возможно присутствует автокорреляция первого порядка, $\e_t=\rho \e_{t-1}+u_t$. При известном числе наблюдений $T$ на уровне значимости 5\% сделайте статистический вывод о наличии автокорреляции.
\begin{enumerate}
\item $T=25$, $k=2$, $DW=0.8$
\item $T=30$, $k=3$, $DW=1.6$
\item $T=50$, $k=4$, $DW=1.8$
\item $T=100$, $k=5$, $DW=1.1$
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
По 100 наблюдениям была оценена модель линейной регрессии
$y_t=\beta_1+\beta_2 x_t+\e_t$. Оказалось, что $RSS=120$, $\he_1=-1$, $\he_{100}=2$, $\sum_{t=2}^{100} \he_t\he_{t-1}=-50$. Найдите $DW$ и $\rho$.


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Применяется ли статистика Дарбина-Уотсона для выявления автокорреляции в следующих моделях
\begin{enumerate}
\item $y_t=\beta_1 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 y_{t-1} + \e_t$
\item $y_t=\beta_1 + \beta_2 t +\beta_3 y_{t-1} + \e_t$
\item $y_t=\beta_1 t + \beta_2 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 t +\beta_3 x_t +\beta_4 x_{t-1} + \e_t$
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
По 21 наблюдению была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(0.3)}{1.2}+\underset{(0.18)}{0.9}\cdot y_{t-1}+\underset{(0.01)}{0.1}\cdot t$, $R^2=0.6$, $DW=1.21$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.


\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
По 24 наблюдениям была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(0.01)}{0.5}+\underset{(0.02)}{2}\cdot t$, $R^2=0.9$, $DW=1.3$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
По 32 наблюдениям была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(2.5)}{10}+\underset{(0.5)}{2.5}\cdot t- \underset{(0.01)}{0.1}\cdot t^2$, $R^2=0.75$, $DW=1.75$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.


\begin{sol}
\end{sol}
\end{problem}




\Closesolutionfile{solution_file}
