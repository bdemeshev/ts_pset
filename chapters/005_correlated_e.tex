% !TEX root = ../ts_pset_main.tex


\chapter{Автокорреляция ошибок в линейной модели}

\begin{problem}
Билл Гейтс оценил модель $y_t=\beta_1 + \beta_2 t + \beta_3 y_{t-1} + \e_t$ с помощью МНК. Значение статистики Дарбина-Уотсона оказалось равно $DW=0.55$. Какой из этого следует вывод об автокорреляции ошибок первого порядка?

\begin{sol}
В данном случае статистика $DW$ не применима, так как есть лаг $y_{t-1}$ среди регрессоров.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим модель $y_t=\beta x_t +\e_t$, где $\e_1=u_1$ и $\e_t=u_t+u_{t-1}$ при $t\geq 2$. Случайные величины $u_i$ независимы с $\E(u_i)=0$ и $\Var(u_i)=\sigma^2$.
\begin{enumerate}
\item Найдите $\Var(\e_t)$
\item Являются ли ошибки $\e_t$ гетероскедастичными?
\item Найдите $\Cov(\e_i,\e_j)$
\item Являются ли ошибки $\e_t$ автокоррелированными?
\item Как выглядит матрица $\Var(\e)$?
\item Рассмотрим оценку
\[
\hb=\frac{\sum x_i y_i}{\sum x_i^2}
\]
Является ли она несмещенной для $\beta$? Является ли она эффективной в классе линейных по $y$ несмещенных оценок?
\item Если приведенная $\hb$ не является эффективной, то приведите формулу для эффективной оценки.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $\E(\e_t)=0$, $\Var(\e_1)=\sigma^2$, $\Var(\e_t)=2\sigma^2$ при $t\geq 2$.  Гетероскедастичная.
\item $\Cov(e_t,e_{t+1})=\sigma^2$. Автокоррелированная.
\item $\hb$ — несмещенная, неэффективная
\item Более эффективной будет $\hb_{gls}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где
\[
X=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
\]

Матрица $V$ известна с точностью до константы $\sigma^2$, но в формуле для $\hb_{gls}$ неизвестная $\sigma^2$ сократится.

Другой способ построить эффективную оценку — применить МНК к преобразованным наблюдениям, т.е. $\hb_{gls}=\frac{\sum x'_i y'_i}{\sum x_i^{\prime 2}}$, где $y'_1=y_1$, $x'_1=x_1$, $y'_t=y_t-y_{t-1}$, $x'_t=x_t-x_{t-1}$ при $t\geq 2$.
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Имеются данные $y=(1,\, 2,\, 0,\,  0,\, 2,\, 1)$. Предполагая модель с автокоррелированной ошибкой, $y_t=\mu+\e_t$, где $\e_t=\rho \e_{t-1}+u_t$ с помощью трёх тестов проверьте гипотезы
$H_0$: $\rho=0$,
$H_0$: $\mu=0$,
$H_0$: $\begin{cases}
\rho=0 \\
\mu = 0 \\
\sigma^2=1
\end{cases}$

\begin{sol}
Для простоты закроем глаза на малое количество наблюдений и как индейцы пираха будем считать, что пять — это много.
\end{sol}
\end{problem}


\begin{problem}
Рассматривается модель $y_t = \mu + \varepsilon_t$, $t = 1,\ldots,T$, где $\varepsilon_t = \rho \varepsilon_{t-1} + u_t$, случайные величины $\varepsilon_0, u_1,\dots,u_T$ независимы, причем $\varepsilon_0 \sim N(0,\sigma^2/(1 - \rho^2))$, $u_t \sim N(0,\sigma^2)$. Имеются наблюдения $y' = (1, 2, 0, 0, 1)$.
\begin{enumerate}
  \item Выпишите функцию правдоподобия
  \[
  \mathrm{L}(\mu, \rho, \sigma^2) = f_{Y_1}(y_1)\prod_{t=2}^{T}f_{Y_t \mid Y_{t-1}}(y_t \mid y_{t-1}).
  \]
  \item Найдите оценки неизвестных параметров модели максимизируя условную функцию правдоподобия
  \[
  \mathrm{L}(\mu, \rho, \sigma^2 \mid Y_1 = y_1) = \prod_{t=2}^{T}f_{Y_t \mid Y_{t-1}}(y_t \mid y_{t-1})
  \]
\end{enumerate}


\begin{sol}
\begin{enumerate}
  \item Поскольку имеют место соотношения $\varepsilon_1 = \rho \varepsilon_0 + u_1$ и $Y_1 =\mu + \varepsilon_1$, то из условия задачи получаем, что $\varepsilon_1 \sim N(0,\sigma^2 / (1 - \rho^2))$
и $Y_1 \sim \cN(\mu,\sigma^2 / (1 - \rho^2))$. Поэтому
\[
f_{Y_1}(y_1) = \frac{1}{\sqrt{2\pi\sigma^2/(1-\rho^2)}}\exp{\left(-\frac{(y_1 - \mu)^2}{2\sigma^2/(1 - \rho^2)}\right)}.
\]

Далее, найдем $f_{Y_2 \mid Y_1}(y_2 \mid y_1)$. Учитывая, что $Y_2 = \rho Y_1 + (1- \rho) \mu + u_2$, получаем $Y_2 \mid \{Y_1 = y_1\} \sim N(\rho y_1 + (1- \rho) \mu, \sigma^2)$. Значит,
\[
f_{Y_2 \mid Y_1}(y_2 \mid y_1) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_2 - \rho y_1 - (1- \rho) \mu)^2}{2\sigma^2}\right)}.
\]

Действуя аналогично, получаем, что для всех $t \geq 2$ справедлива формула
\[
f_{Y_{t} \mid Y_{t-1}}(y_{t} \mid y_{t-1}) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_{t} - \rho y_{t-1} - (1- \rho) \mu)^2}{2\sigma^2}\right)}.
\]

Таким образом, находим функцию правдоподобия
\[
\mathrm{L}(\mu, \rho, \sigma^2) = f_{Y_T,\ldots,Y_1}(y_T,\dots,y_1) = f_{Y_1}(y_1)\prod_{t=2}^{T}f_{Y_t \mid Y_{t-1}}(y_t \mid y_{t-1}) ,
\]
где $f_{Y_1}(y_1)$ и $f_{Y_t \mid Y_{t-1}}(y_t \mid y_{t-1})$ получены выше.

\item Для нахождения неизвестных параметров модели запишем логарифмическую условную функцию правдоподобия:
\[
\ln L(\mu, \rho, \sigma^2 \mid Y_1 = y_1) = \sum_{t=2}^{T}\log{f_{Y_t \mid Y_{t-1}}(y_t \mid y_{t-1})} =
\]
\[
=-\frac{T-1}{2} \log(2 \pi) - \frac{T-1}{2} \log{\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^{T}(y_t - \rho y_{t-1} - (1 - \rho) \mu)^2 .
\]

Найдем производные функции $\ell(\mu, \rho, \sigma^2 \mid Y_1 = y_1)$ по неизвестным параметрам:
\[
\frac{\partial \ell}{\partial \mu} = -\frac{1}{2\sigma^2} \sum_{t=2}^{T} 2(y_t - \rho y_{t-1} - (1 - \rho) \mu) \cdot (\rho - 1) ,
\]
\[
\frac{\partial \ell}{\partial \rho} = -\frac{1}{2\sigma^2} \sum_{t=2}^{T} 2(y_t - \rho y_{t-1} - (1 - \rho) \mu) \cdot (\mu - y_{t-1}) ,
\]
\[
\frac{\partial \ell}{\partial {\sigma^2}} =  - \frac{T-1}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{t=2}^{T}(y_t - \rho y_{t-1} - (1 - \rho) \mu)^2 .
\]

Оценки неизвестных параметров модели могут быть получены как решение следующей системы уравнений:
\[
\begin{cases}
    \frac{\partial \ell}{\partial \mu} = 0, \\
    \frac{\partial \ell}{\partial \rho} = 0,\\
    \frac{\partial \ell}{\partial {\sigma^2}} = 0.
\end{cases}
\]

Из первого уравнения системы получаем, что
\[
\sum_{t=2}^{T}y_{t} - \hat{\rho} \sum_{t=2}^{T}y_{t-1} = (T - 1) (1- \hat{\rho}) \hat{\mu} ,
\]
откуда
\[
\hat{\mu} = \frac{\sum_{t=2}^{T}y_{t} - \hat{\rho} \sum_{t=2}^{T}y_{t-1}}{(T - 1) (1- \hat{\rho})} = \frac{3 - \hat{\rho} \cdot 3}{4\cdot(1-\hat{\rho})} = \frac{3}{4} .
\]

Далее, если второе уравнение системы переписать в виде
\[
\sum_{t=2}^{T}(y_t - \hat{\mu} - \hat{\rho} (y_{t-1} - \hat{\mu}))(y_{t-1} - \hat{\mu}) = 0 ,
\]
то легко видеть, что
\[
\hat{\rho} = \frac{\sum_{t=2}^{T}(y_t - \hat{\mu})(y_{t-1} - \hat{\mu})}{\sum_{t=2}^{T}(y_{t-1} - \hat{\mu})^2} .
\]
Следовательно, $\hat{\rho} =-1/11= -0.0909$.

Наконец, из третьего уравнения системы
\[
\hs^2 =\frac{1}{T-1} \sum_{t=2}^{T}(y_t - \hat{\rho} y_{t-1} - (1 - \hat{\rho}) \hat{\mu})^2 .
\]
Значит, $\hs^2 = 165/242= 0.6818$. Ответы: $\hat{\mu} = 3/4= 0.75$, $\hat{\rho} = -1/11=-0.0909$, $\hs^2 =165/242=0.6818$.
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
Остаются ли в условиях автокорреляции МНК-
оценки в линейной модели несмещёнными? Состоятельными?
\begin{sol}
Несмещёнными остаются. Состоятельными не всегда остаются, например, состоятельность исчезает, если все случайные ошибки тождественно равны между собой.

\end{sol}
\end{problem}



\begin{problem}
Продавец мороженного оценил динамическую модель объёмов продаж:
\[
\ln \hat{Q}_t=26.7 + 0.2\ln \hat{Q}_{t-1}-0.6\ln P_t
\]
Здесь $Q_t$ — число проданных в день $t$ вафельных стаканчиков, а $P_t$ — цена одного стаканчика в рублях. Продавец также рассчитал остатки $\hat{e}_t$.
\begin{enumerate}
\item Чему, согласно полученным оценкам, равна долгосрочная эластичность объёма продаж по цене?
\item Предположим, что продавец решил проверить наличие автокорреляции первого порядка с помощью теста Бройша-Годфри. 
Выпишите уравнение регрессии, которое он должен оценить.
\end{enumerate}


\begin{sol}
  \begin{enumerate}
    \item Для начала мы избавимся от логарифмов.
  \[
    \ln \hat{Q}_t = 26.7 + 0.2 \ln \hat{Q}_{t-1} - 0.6\ln P_t 
  \Leftrightarrow 
  \ln \hat{Q}_t = \ln \left(e^{26.7} \cdot \hat{Q}_{t-1} ^ {0.2} \cdot P_t ^ {-0.3}\right) \Leftrightarrow 
  \hat{Q}_t = e^{26.7} \cdot \hat{Q}_{t-1} ^ {0.2} \cdot P_t ^ {-0.3}
  \]
  Воспользуемся формулой эластичности объема продаж по цене (спроса по цене).
  \[
    \hat{e}_t = \frac{\frac{\Delta Q(p)}{Q(p)}}{\frac{\Delta P}{P}} = \frac{(e^{26.7} \cdot Q_{t-1} ^ {0.2} \cdot P_t ^ {-0.3})'}{(P)'} = \frac{-0.3 P ^ {1.3}(e^{26.7} \cdot Q_{t-1} ^ {0.2})}{1} = -0.3 P ^ {1.3}(e^{26.7} \cdot Q_{t-1} ^ {0.2})
  \]
  \item По формуле теста Бройша-Годфри
  \[
    e_t = \sum_{i = 1}^p e_i \cdot a_{t-i} + u_t
  \]
  То есть для нашего случая $p = 1$, так как $p$ равна порядку автокорелляции, то есть единице,
  \[
    e_t =  e_{t-1} \cdot a_{1} + u_t.
  \]
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Пусть $u_t$ — независимые нормальные случайные величины с
математическим ожиданием $0$ и дисперсией $\sigma^2$. Известно, что $\e_1=u_1$, $\e_t=u_1+u_2+\ldots+u_t$. 
Рассмотрим модель $y_t=\beta_1+\beta_2 x_t + \e_t$.

\begin{enumerate}
\item Найдите $\Var(\e_t)$, $\Cov(\e_t,\e_s)$, $\Var(\e)$
\item Являются ли ошибки $\e_t$ гетероскедастичными?
\item Являются ли ошибки $\e_t$ автокоррелированными?
\item Предложите более эффективную оценку вектора коэффициентов регрессии по сравнению МНК-оценкой.
\item Результаты предыдущего пункта подтвердите симуляциями Монте-Карло на компьютере.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Ошибки в модели $y_t=\beta_1+\beta_2 x_{t}+\e_t$ являются автокоррелированными первого порядка, $\e_t=\rho \e_{t-1}+u_t$. Шаман-эконометрист Ойуун выполняет два камлания-преобразования. Поясните смысл камланий:
\begin{enumerate}
\item Камлание А, при $t\geq 2$, Ойуун преобразует уравнение к виду $y_t-\rho y_{t-1}=\beta_1(1-\rho)+ \beta_2(x_t-\rho x_{t-1})+\e_t-\rho \e_{t-1}$
\item Камлание Б, при $t=1$, Ойуун преобразует уравнение к виду $\sqrt{1-\rho^2}y_1=\sqrt{1-\rho^2}\beta_1+\sqrt{1-\rho^2}\beta_2 x_1+\sqrt{1-\rho^2}\e_1$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим модель $y_t=\beta_1+\beta_2 x_{t1}+\ldots+\beta_k x_{tk}+\e_t$, где $\e_t$ подчиняются автокорреляционной схеме первого порядка, т.е.
\begin{enumerate}
\item $\e_t=\rho \e_{t-1}+u_t$, $-1<\rho<1$
\item $\Var(\e_t)=const$, $\E(\e_t)=const$
\item $\Var(u_t)=\sigma^2$, $\E(u_t)=0$
\item Величины $u_t$ независимы между собой
\item Величины $u_t$ и $\e_s$ независимы, если $t\geq s$
\end{enumerate}
Найдите:
\begin{enumerate}
\item $\E(\e_t)$, $\Var(\e_t)$
\item $\Cov(\e_t,\e_{t+h})$
\item $\Corr(\e_t,\e_{t+h})$
\end{enumerate}

\begin{sol}
$
\E(e_t) = \E(\rho e_{t-1}+u_t) = \rho \E( e_{t-1})+E(u_t) = \rho \E( e_{t-1})$. При условии, что $\E(e_t)=\text{const}$, получаем:
\[
x = \rho x \rightarrow (\rho - 1)x = 0 \rightarrow x = 0, \rho < 1 \rightarrow E(e_t) = 0
\]
\[
\Var(e_t) = \Var(\rho e_{t-1}+u_t) = \Var(\rho e_{t-1})+ \Var(u_t) + 2\Cov(\rho e_{t-1}, u_t) = \rho^2 \Var(e_{t-1})+ \sigma^2 
\]
при условии, что $\Var(e_t)=\text{const}$, получаем:
\[
x = \rho^2 x + \sigma^2 \rightarrow x = \frac{\sigma^2}{1 - \rho^2} \rightarrow Var(e_t) = \frac{\sigma^2}{1 - \rho^2}
\]
\begin{align*} 
\Cov(e_t,e_{t+h}) = \Cov(e_t,\rho e_{t+h-1}+u_{t+h}) =
\Cov(e_t,\rho (\rho e_{t+h-2}+u_{t+h-1})+u_{t+h}) = \Cov(e_t,\rho^{h} e_{t}+\sum_{i=t+1}^{t+h} \rho^{t+h-i}u_{i}) = \\
= \Cov(e_t,\rho^{h} e_{t})+\Cov(e_t, \sum_{i=t+1}^{t+h} \rho^{t+h-i}u_{i}) = \Cov(e_t,\rho^{h} e_{t}) = \rho^{h} \Var(e_t) =
\rho^{h} \frac{\sigma^2}{1 - \rho^2}
\end{align*}
\[
Corr(e_t, e_{t+h}) = \frac{\Cov(e_t,e_{t+h})}{\sqrt{\Var(e_t)} \sqrt{Var(e_{t+h})}} = \frac{\Cov(e_t,e_{t+h})}{\Var(e_t)} = \frac{\rho^{h} \frac{\sigma^2}{1 - \rho^2}}{\frac{\sigma^2}{1 - \rho^2}} = \rho^{h}
\]
Ответ:
\begin{enumerate}
\item $\E(\e_t)=0$, $\Var(\e_t)=\sigma^2/(1-\rho^2)$
\item $\Cov(\e_t,\e_{t+h})=\rho^h\cdot \sigma^2/(1-\rho^2)$
\item $\Corr(\e_t,\e_{t+h})=\rho^h$
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Рассматривается модель $y_t=\beta_1+\beta_2 x_{t1}+\ldots+\beta_k x_{tk}+\e_t$. Ошибки $\e_t$ гомоскедастичны, но в них возможно присутствует автокорреляция первого порядка, $\e_t=\rho \e_{t-1}+u_t$. 

При известном числе наблюдений $T$ на уровне значимости 5\% сделайте статистический вывод о наличии автокорреляции.
\begin{enumerate}
\item $T=25$, $k=2$, $DW=0.8$
\item $T=30$, $k=3$, $DW=1.6$
\item $T=50$, $k=4$, $DW=1.8$
\item $T=100$, $k=5$, $DW=1.1$
\end{enumerate}


\begin{sol}
\begin{enumerate}
  \item Воспользуемся табличкой критических значений $DW$. 
  Для $T = 25, k = 2$ критические значения для этого количества: $d_L = 1.21, d_U = 1.55$. 
  Гипотеза об отсутствии автокорреляции отвергается.
  \item Воспользуемся табличкой критических значений $DW$. 
  Для $T = 30, k = 3$ критические значения для этого количества: $d_L = 1.21, d_U = 1.65$. 
  Статистика $DW$ попадает в зону неопределенности. 
  \item Воспользуемся табличкой критических значений $DW$. 
  Для $T = 50, k = 4$ критические значения для этого количества: $d_L = 1.38, d_U = 1.72$. 
  Гипотеза об отсутствии автокорреляции не отвергается.
 \item  Воспользуемся табличкой критических значений $DW$. 
  Для $T = 100, k = 5$ критические значения для этого количества: $d_L = 1.57, d_U = 1.78$. 
  Статистика $DW$ меньше  $d_L$ и гипотеза об отсутствии автокорреляции отвергается.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
По 100 наблюдениям была оценена модель линейной регрессии $y_t=\beta_1+\beta_2 x_t+\e_t$. 
Оказалось, что $RSS=120$, $\he_1=-1$, $\he_{100}=2$, $\sum_{t=2}^{100} \he_t\he_{t-1}=-50$. 

Найдите $DW$ и $\rho$.

\begin{sol}
  Вспомним формулу теста Дарбина-Уотсона
\[
  DW = \frac{\sum_{i = 2}^n (\hat{e}_i - \hat{e}_{i - 1})^2}{\sum_{i = 1}^n \hat{e}_i^2}
\]
Осталось ее применить. 
\begin{align*}
DW = \frac{\sum_{i = 2}^n (\hat{e}_i - \hat{e}_{i - 1})^2}{\sum_{i = 1}^n \hat{e}_i^2} = \frac{\sum_{i = 2}^n \hat{e}_i^2}{\sum_{i = 1}^n \hat{e}_i^2} + \frac{\sum_{i = 1}^{n-1} \hat{e}_i^2}{\sum_{i = 1}^n \hat{e}_i^2}  + \frac{\sum_{i = 2}^n \hat{e}_i \cdot \hat{e}_{i - 1}}{\sum_{i = 1}^n \hat{e}_i^2}=\\ 
= \frac{\sum_{i = 1}^n \hat{e}_i^2 - \hat{e_1}^2}{\sum_{i = 1}^n \hat{e}_i^2} + \frac{\sum_{i = 1}^{n} \hat{e}_i^2 - \hat{e_{100}}^2}{\sum_{i = 1}^n \hat{e}_i^2}  - \frac{\sum_{i = 2}^n \hat{e}_i \cdot \hat{e}_{i - 1}}{\sum_{i = 1}^n \hat{e}_i^2}=\\
=  \frac{120 - (-1)^2}{120} + \frac{120 - (2)^2}{120} - \frac{-50}{120} =\\
= \frac{119}{120} + \frac{116}{120} + \frac{50}{120} = \frac{285}{120} = 2.375
\end{align*}
Теперь найдем $\rho$
\[
  \rho = \frac{\sum_{i = 2}^n \hat{e}_i \cdot \hat{e}_{i - 1}}{\sum_{i = 1}^n \hat{e}_i^2} = \frac{-50}{120} \approx -0.417
\]
\end{sol}
\end{problem}



\begin{problem}
Применяется ли статистика Дарбина-Уотсона для выявления автокорреляции в следующих моделях
\begin{enumerate}
\item $y_t=\beta_1 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 y_{t-1} + \e_t$
\item $y_t=\beta_1 + \beta_2 t +\beta_3 y_{t-1} + \e_t$
\item $y_t=\beta_1 t + \beta_2 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 t +\beta_3 x_t +\beta_4 x_{t-1} + \e_t$
\end{enumerate}


\begin{sol}
  Для начала вспомним, когда мы не можем применить статистику Дарбина Уотсона:
  \begin{enumerate}
      \item если в уравнении нет свободного члена,
      \item если в уравнении есть стохастический регрессор,
      \item если возмущения удовлетворяют авторегрессионной схеме не первого, а большего порядка.
  \end{enumerate}
  \begin{enumerate}
    \item В этом уровнении нет свободного члена, статистика Дарбина Уотсона не применима.
   \item Для этой модели ни одно из условий неприменимости не выполняется, так что можно применить статистику Дарбина-Уотсона.
  \item В уравнении есть стохастический регрессор, статистика Дарбина Уотсона не применима.
  \item В уравнении есть стохастический регрессор, статистика Дарбина Уотсона не применима.
  \item В этом уровнении нет свободного члена, статистика Дарбина Уотсона не применима.
  \item Для этой модели ни одно из условий неприменимости не выполняется, так что можно применить статистику Дарбина-Уотсона.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
По 21 наблюдению была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(0.3)}{1.2}+\underset{(0.18)}{0.9}\cdot y_{t-1}+\underset{(0.01)}{0.1}\cdot t$, $R^2=0.6$, $DW=1.21$. 

Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.

\begin{sol}
  Воспользуемся табличкой критических значений $DW$. 
Для $T = 21, k = 2$ критические значения для этого количества: $d_L = 1.13, d_U = 1.54$. 
Статистика $DW$ попадает в зону неопределенности.
\end{sol}
\end{problem}



\begin{problem}
По 24 наблюдениям была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(0.01)}{0.5}+\underset{(0.02)}{2}\cdot t$, $R^2=0.9$, $DW=1.3$. 

Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.

\begin{sol}
  Воспользуемся табличкой критических значений $DW$. 
Для $T = 24, k = 1$ критические значения для этого количества: $d_L = 1.27, d_U = 1.45$. 
То есть $DW$ попадает в зону неопределенности.
\end{sol}
\end{problem}



\begin{problem}
По 32 наблюдениям была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(2.5)}{10}+\underset{(0.5)}{2.5}\cdot t- \underset{(0.01)}{0.1}\cdot t^2$, $R^2=0.75$, $DW=1.75$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.

\begin{sol}
  Воспользуемся табличкой критических значений $DW$. 
Для $T = 32, k = 2$ критические значения для этого количества: $d_L = 1.31, d_U = 1.57$. 
То есть $DW$ меньше  $d_U$ и гипотеза об отсутствии автокорреляции не отвергается.
\end{sol}
\end{problem}


